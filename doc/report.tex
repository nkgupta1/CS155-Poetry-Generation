\newif\ifshowsolutions
\showsolutionstrue
\input{preamble}
\newcommand{\boldline}[1]{\underline{\textbf{#1}}}

\chead{%
  {\vbox{%
      \vspace{2mm}
      \large
      Machine Learning \& Data Mining \hfill
      Caltech CS/CNS/EE 155 \hfill \\[1pt]
      Miniproject 1\hfill
      Released February $17^{th}$, 2017 \\
    }
  }
}

\usepackage{amsfonts} %Mathbb Fonts
\usepackage{amsmath} %Text in Math
\usepackage{longtable} %Make multi-page tables
\usepackage{enumitem}
\usepackage{graphicx} % Import pics \includegraphics[scale=0.7]{SGD}
\graphicspath{{figures/}} % Change pic path
\usepackage{makecell}
\usepackage[margin=2.25cm]{caption}

\begin{document}
\pagestyle{fancy}

\section{Introduction}
\medskip
\begin{itemize}

    \item \boldline{Group members} \\
    Vaibhav Anand \\
    Nikhil Gupta \\
    Michael Hashe
    
    \item \boldline{Team name} \\
    The Breakfast Club

    \item \boldline{GitHub link} \\
    \href{https://github.com/nkgupta1/CS155-Project2-Poetry-Generation}{https://github.com/nkgupta1/CS155-Project2-Poetry-Generation}
    
    \item \boldline{Division of labour} \\
    Vaibhav Anand: \\
    Nikhil Gupta: \\
    Michael Hashe: 

\end{itemize}

\section{Tokenizing}
\medskip

\subsection{What methods did you use and try to tokenize the sonnets?}
\subsection{Did you have to make changes to the way you tokenized after running the algorithm and seeing the results?}


\section{Algorithm}
\medskip

\subsection{What packages did you use for the algorithm?}
\subsection{What decisions did you have to make when running the algorithm and what did you try? e.g number of states}
\subsection{How did this affect the sonnets that were generated}

\section{Poetry Generation}
\medskip

\subsection{How did you generate your poem?}
\subsection{How did you get your poem to look as much like a sonnet as possible?}
\subsection{What makes sense/what doesn't about the sonnets generated?}


\section{Visualization and Interpretation}
\medskip

\subsection{For at least 5 hidden states give a list of the top 10 words that associate with this hidden state and
state any common features these groups.}
\subsection{What are some properties of the different hidden states?}
\subsection{e.g. Correlation between hidden states and syllable counts, connotations of words, etc.}
\subsection{Make a visual representation of the correlation between states and words}


\section{Additional Goals}
\medskip

\subsection{e.g. rhyme/meter}


\section{Extra Credit}
\medskip

\subsection{What did you use to collect more data and what packages did you use for RNN/LSTM?}
We used both the data sets, Shakespeare and Spenser. We did not any data in addition to these two sets of data. We used Keras with a TensorFlow backend for the RNN/LSTM.
\subsection{Compare/contrast the effect of these algorithms to HMM and why you think they were better or worse}
The RNN was much more finicky than the HMM was in that it was much more sensitive to parameters than the HMM was. In particular, if a network was under trained, all it did was output a single word, usually \texttt{the}. The three main parameters we varied over the different networks was the architecture (we tried 64x32, 128x64, 128x128, 256x128, 256x256, 1024x256), the number of iterations of training (the optimal number varied based on the network architecture), and the input sequence length. If the network was over trained, it usually repeated a short phrase or just memorized passages from the original text. As such, there was a very small range where the network generated a novel, potentially good output. Furthermore, smaller networks just repeated the same short phrase over and over again while the larger networks just repeated larger phrases if not just memorizing full passages from the data set. \\
\indent We believe the reason that we had more success with the HMM is that in general, RNNs are difficult to train, especially on the limited quantity of data that we had. Neural networks in general require a large amount of data to train since they have many parameters. To combat this, we limited the depth of the network, and therefore the number of parameters which meant we were limiting the power of the RNN. While RNNs have much more potential, the data set was too small to train a good model. As such, HMMs were better on the smaller data set, even though they do not have as much potential as a RNN because they can only look one state back. The RNN took much longer to train than the HMM (1.5 minutes per iteration on average vs. 10 seconds per iteration) . \\
\indent We fed input into the network character by character in order to reduce the input size. We made all characters lowercase to further reduce input. We trained the RNN in an unsupervised manner. In order to generate output, we seeded the model with sequence of characters selected from the sonnets, which when fed into the network, generated the next character. We appended this character to the input sequence, removing one of the other characters and fed it back into the network, repeating until we got a poem of the desired length. \\
\indent Here is an example of an excerpt generated from a model that was overfit to the data (architecture of 1024x256 trained for 150 iterations): \\
\texttt{
    i move to hear her speak, \\
    yet well i know, \\
    that music hath a far more pleasing sound: \\
    i grant i never saw a goddess go, \\
    my mistress when she walks treads on the ground. \\
    and yet by heaven i think my love as lare \\
} \\
which is just a copy of Shakespeare's sonnet 130. One of the more successful generations we got is the following, generated with 1024x256 with 20 iterations: \\
\texttt{
    That love in move that I am stained, \\
    And thet in thee art shat I have seen she torld and hr thee, \\
    And therefore hand that thou aestaintance, \\
    And thet in thee I say tort the eane, \\
    So shall thou that which I have steared, \\
    The have the len of mort of thine eyes, \\
    And in the world with shale the love, \\
    Oo marker then thou art as the world and wouth, \\
    And in the world would may still so more, \\
    And yours toue love wou are io heaven shall stay, \\
    Which in their sarte of think the soues, \\
    And therefore hand that thou aestaintance, \\
    And thet in thee I say tort the eane, \\
    So shall thou that which I have steared, \\
    The have the len of mort of thine eyes. \\
}


\section{Conclusion}
\medskip

\subsection{How was the work divided up?}
\subsection{What are your conclusions/observations about the models you used and the sonnets generated?}



\end{document}